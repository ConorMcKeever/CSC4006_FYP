Looking For Bugs in All the Right Places

Robert M. Bell, Thomas J. Ostrand, Elaine J. Weyuker
AT&T Labs - Research
180 Park Avenue
Florham Park, NJ 07932
(rbell,ostrand,weyuker)@research.att.com

ABSTRACT
We continue investigating the use of a negative binomial re-
gression model to predict which (cid:12)les in a large industrial
software system are most likely to contain many faults in
the next release. A new empirical study is described whose
subject is an automated voice response system. Not only is
this system’s functionality substantially di(cid:11)erent from that
of the earlier systems we studied (an inventory system and
a service provisioning system), it also uses a signi(cid:12)cantly
di(cid:11)erent software development process.
Instead of having
regularly scheduled releases as both of the earlier systems
did, this system has what are referred to as (cid:2)continuous re-
leases.(cid:1) We explore the use of three versions of the negative
binomial regression model, as well as a simple lines-of-code
based model, to make predictions for this system and discuss
the di(cid:11)erences observed from the earlier studies. Despite the
di(cid:11)erent development process, the best version of the pre-
diction model was able to identify, over the lifetime of the
project, 20% of the system’s (cid:12)les that contained, on average,
nearly three quarters of the faults that were detected in the
system’s next releases.
Categories and Subject Descriptors: D.2.5 [Software
Engineering]: Testing and Debugging (cid:8) Debugging aids
General Terms: Experimentation
Keywords: software faults, fault-prone, prediction, regres-
sion model, empirical study, software testing

1.

INTRODUCTION AND PRIOR WORK
Since substantial resources are expended in e(cid:11)orts to make
large industrial software systems highly dependable, we have
been developing techniques to accurately predict which (cid:12)les
in a software system are most likely to be problematic in the
next release. That information should help alert software
developers to (cid:12)les that may need to be rewritten or to which
particular care should be paid and help testers to prioritize
and focus their e(cid:11)orts.

Permission to make digital or hard copies of all or part of this work
for personal or classroom use is granted without fee provided that
copies are not made or distributed for proﬁt or commercial advantage
and that copies bear this notice and the full citation on the ﬁrst page.
To copy otherwise, to republish, to post on servers or to redistribute
to lists, requires prior speciﬁc permission and/or a fee.
ISSTA’06, July 17-20, 2006, Portland, Maine, USA.
Copyright 2006 ACM 1-59593-263-1/06/0007 ...$5.00.

In earlier papers [18, 19] we introduced a negative bino-
mial regression model that we used to accurately predict
which (cid:12)les are likely to contain the largest numbers of faults
in the next release, and which (cid:12)les are likely to account for a
pre-speci(cid:12)ed percentage of the faults in the next release [20].
These predictions were based on (cid:12)le characteristics that
could be objectively assessed, including the size of the (cid:12)le in
terms of the number of lines of code (LOC), whether this was
the (cid:12)rst release in which the (cid:12)le appeared, whether (cid:12)les that
occurred in earlier releases had been changed or remained
unchanged from the previous release, how many previous
releases the (cid:12)le occurred in, how many faults were detected
in the (cid:12)le during the previous release, and the programming
language in which the (cid:12)le was written. We selected these
characteristics because our initial studies indicated that they
were the most relevant ones [16].

We performed two case studies using two di(cid:11)erent large
industrial software systems: an inventory system and a ser-
vice provisioning system. Each of these systems ran contin-
uously, and had been in use for multiple years. For both of
the systems we found our model was able to make very accu-
rate predictions by associating a predicted number of faults
with each (cid:12)le, and sorting the (cid:12)les in decreasing order of
the number of predicted faults. For both systems, when we
selected the 20% of the (cid:12)les predicted to contain the largest
numbers of faults, they contained, on average, 83% of the
actual faults that were detected in the next release.

We also considered a highly simpli(cid:12)ed prediction model
for both the inventory and provisioning systems that was
based solely on the number of lines of code in (cid:12)les. We
found that for the inventory system, the 20% of the (cid:12)les
identi(cid:12)ed by the LOC Model contained, on average, 73%
of the faults, while for the provisioning system those (cid:12)les
contained 74% of the faults. Thus, we found that even a
very highly simpli(cid:12)ed prediction model could provide value,
although the full model always identi(cid:12)ed a larger percentage
of the faults.

Details of these studies can be found in [18, 19]. In those
studies, as well as in this paper, the phrase actual faults al-
ways refers to the faults that have been detected. Obviously,
a system may contain undetected faults, but there is no way
for us to be aware of them.

In this paper we continue our investigation using a sub-
stantially di(cid:11)erent type of system with very di(cid:11)erent charac-
teristics. This is an automated voice response system which
is used by many companies to provide customer service while
limiting reliance on human operators. The most signi(cid:12)cant
di(cid:11)erentiating characteristic of this system is the lack of a

formal release schedule, which made the straightforward ap-
plication of our earlier models problematic.

All three systems used an integrated version control/change
management system that required a modi(cid:12)cation request or
MR to be written any time a change was to be made to
the system. An MR, which is most commonly written by
a developer or tester, may identify either (1) a problem or
issue found during internal project testing or reported by a
customer or (2) a required or requested change, such as a
system enhancement or maintenance update.

MRs contain a great deal of information, including a writ-
ten description of the reason for the proposed change and a
severity rating of 1 through 4 characterizing the importance
of the proposed change. If the request results in an actual
change, the MR records the (cid:12)le(s) that are changed or added
to the system and the speci(cid:12)c lines of code that are added,
It also includes such information as
deleted, or modi(cid:12)ed.
the date of the change and the development stage at which
the change was made.

Most projects begin MR data collection at the time that
the system test phase begins for the (cid:12)rst release, and this
was the case for the provisioning system used in our case
studies. The inventory system began data collection far ear-
lier, at the requirements stage, and almost three quarters of
the reported faults were identi(cid:12)ed during unit testing. Un-
like system testing, which is typically done by professional
testers whose sole job function is to develop and run test
cases once the system has been fully integrated, unit testing
is generally performed by developers while they are creating
individual (cid:12)les. In addition, the system test process tends to
be far more carefully controlled than the unit testing phase.
Prior to the start of system testing, the software is typ-
ically viewed as a (cid:2)work in progress,(cid:1) and therefore it is
very unusual for a project to begin entering code and MRs
into the formal version control/change management system
prior to system test. For that reason, we wanted to verify
that the prediction results observed for the inventory system
were not a(cid:11)ected by this unusual behavior, and so we also
used the model to make predictions after removing all MRs
that were associated with faults found during unit test and
earlier. When we restricted the data included and analyzed
in this way, the results, averaged over the seventeen releases
representing four years of (cid:12)eld exposure, were almost the
same. In particular, the 20% of the (cid:12)les identi(cid:12)ed by the
model contained, on average, 84% of the faults.

Section 2 of this paper describes related research per-
formed by other groups. In Section 3, we provide details of
the voice response system, and point out some of the char-
acteristics that make it particularly di(cid:11)erent, and therefore,
interesting to study. In Section 4 we discuss a solution that
we used to deal with the lack of a formal release schedule so
that we could build a model and make predictions. Section 5
describes the negative binomial regression methodology and
the predictor variables we used. Section 6 describes the vari-
able selection process and presents the resulting models. In
Section 7, we assess prospective predictions from four alter-
native models and compare them with our earlier results.
Section 8 summarizes the main (cid:12)ndings and presents our
conclusions.

2. RELATED WORK BY OTHER GROUPS
Researchers have attempted to identify software proper-
ties that correlate with fault-prone code for many years.

Some of this work is reported in [1, 2, 4, 5, 7, 8, 13, 14,
16, 21]. Attempts to predict the probability, the locations
or the quantity of future faults in code are more recent, and
include [3, 6, 9, 10, 12, 15] as well as our research described
in [18, 19, 20].

Mockus and Weiss [12] studied maintenance requests made
for a large telephone switching system, with the goal of con-
structing a model to predict the probability that the soft-
ware changes made in response to a request will cause a
failure in the modi(cid:12)ed system. They based their model on a
history of about 15,000 maintenance requests over a period
of ten years. Their model’s predictors are characteristics of
the change that are available immediately after the coding
for the change has been completed. These include the size of
the change; the number of distinct (cid:12)les, modules, and sub-
systems that are touched by the change; the time duration
needed to complete the change; the number of developers
involved in making the change; and the experience with the
system of the programmers who made the change.

Their model is constructed using logistic regression, and
does not predict which parts of the code are most likely
to contain faults. Instead it predicts either failure or non-
failure for a given maintenance request. The model, im-
plemented as a web-based tool available to project man-
agement, is used to help schedule the implementation of a
given maintenance request, and to determine the level of
testing resources to apply to validate the implementation.
The paper presents the regression formulas used to create
the prediction model and describes the use of the tool in a
software maintenance environment, but does not report a
success rate for the model’s predictions.

Graves et al. [6] performed a study to determine char-
acteristics of modules that are associated with faults, and
constructed and evaluated several models for predicting the
number of faults that would appear in a future version of
the modules. Their study used the fault history of a large
telecommunications system containing approximately 1.5 mil-
lion LOC, organized into 80 modules, containing a total of
about 2500 (cid:12)les. The prediction models were used to make
fault predictions for a single two year time interval, based
on the system’s history for the preceding two years. They
found that module size was a poor predictor of fault like-
lihood, while the most accurate predictors included combi-
nations of the module’s age, the number of changes made,
and the ages of the changes. The authors also described the
application of their models to a one year interval in the mid-
dle of the original two year interval and found that certain
parameter values di(cid:11)ered by an order of magnitude between
the two time periods. Our results in [19] and in the present
paper partly agree and partly con(cid:13)ict with [6]. File age and
previous changes were positive indicators of fault-proneness
in our studies as well as that of [6], but in contrast to [6],
we have consistently found (cid:12)le size to be a strong predictor
of fault-proneness.

Denaro and Pezze [3] use logistic regression to construct
software fault prediction models based on sets of software
metrics. Their method constructs a very large number of
candidate models (over 500,000 based on sets of up to (cid:12)ve
metrics chosen from a list of 38) and then chooses the best
models by evaluating how close each model comes to cor-
rectly identifying all the fault-prone and non-fault-prone
modules in the system. A module is de(cid:12)ned to be fault-prone
if it belongs to the smallest set of modules that are collec-

tively responsible for 80% of all the faults in the system.
Their experiment uses the publicly available code and fault
databases of the Apache web server. The models are con-
structed using data from Apache version 1.3 and evaluated
for their prediction ability on Apache version 2.0. When the
modules of Apache 2.0 are ordered according to decreasing
fault occurrence, approximately 36% of the modules account
for 80% of the actual faults. The best models of [3] require
the (cid:12)rst 50% of the predicted fault-prone modules to capture
80% of the actual faults.

Work by Khoshgoftaar, Allen, and Deng [10] presents a
method to construct a binary decision tree to classify mod-
ules of a system as fault-prone or not fault-prone. The tree’s
decision nodes are based on 24 static software metrics and
four execution time metrics. The approach is demonstrated
with four releases of a large legacy telecommunications sys-
tem. The regression-tree construction algorithm of the S-
Plus statistics package is applied to training data from the
(cid:12)rst release, producing the tree that is then evaluated on
data of the next three releases.

The authors evaluate the success of the method according
to the misclassi(cid:12)cation rates of fault-prone modules as non-
fault-prone, and non-fault-prone modules as fault-prone. Al-
though it is desirable for both rates to be as low as possible,
there is generally an inverse relation between the frequen-
cies of the two types of misclassi(cid:12)cations. The method’s
user can select the value of a parameter to control the rates
and then choose the value that comes closest to the soft-
ware project’s needs. For instance, if run-time failures are
considered intolerable, then misclassi(cid:12)cations of fault-prone
modules could be lowered, at the expense of predicting more
fault-free modules to be fault-prone, and hence increasing
the time required for thorough testing. Choosing a param-
eter value that attempts to keep the two misclassi(cid:12)cation
rates roughly the same resulted in rates of 32.2% and 21.7%
on the last of the four releases evaluated. When applied to a
real system, the technique will be valuable to the extent that
the set of predicted fault-prone modules contains a higher
number of actual faulty modules than an equal-size random
selection of modules. The authors calculate that applying
their model to a hypothetical future release with the same
percentage of faulty modules as release 4 would more than
double the number of actually faulty modules selected, in
comparison to a random set.

3. THE SYSTEM UNDER STUDY

Both the inventory system and the provisioning system
used in our earlier empirical studies had been relatively ma-
ture when we (cid:12)rst began studying them. The inventory
system had been in the (cid:12)eld for roughly three years (twelve
quarterly releases) when we began data collection and anal-
ysis, while the provisioning system had had nine releases
and roughly two years of (cid:12)eld use. We continued to study
the inventory system for an additional (cid:12)ve releases, yielding
a total of seventeen releases done over a period of roughly
four years. We were now particularly interested in (cid:12)nding an
appropriate system that was earlier in its lifetime, with the
hope of actually in(cid:13)uencing the project’s behavior by pro-
viding predictions that could help to prioritize the project’s
testing e(cid:11)ort.

The voice response system was a prime candidate to inves-
tigate. It had been under development for approximately 18
months, was already being used by a number of companies

in di(cid:11)erent industries, exposing it to a substantial amount of
tra(cid:14)c and a variety of usage modes, and was still undergo-
ing active enhancement and modi(cid:12)cation. The project also
used the same version control/change management system
as the earlier projects, allowing us to make use of the data
mining experience we had gained with this tool while doing
the earlier studies.

Because one of our biggest previous problems with data
collection had been the accurate identi(cid:12)cation of MRs that
represented faults, we requested that this project modify
the MR form to include an explicit bug identi(cid:12)cation (cid:12)eld.
The modi(cid:12)ed MR form became available during Month 21
of the data collection phase, and gave users three choices to
identify an MR: bug, not a bug, and don’t know.

To classify MRs as either fault or non-fault, we used this
(cid:12)eld in combination with an existing MR (cid:12)eld called MR-
Category that describes the reason for creating the MR. This
(cid:12)eld can take on values such as developer-found, system-
test-found, customer-found, new-code, and new-feature. For
our analysis, we restricted attention to only those MRs whose
categories were either system-test-found or customer-found,
because the system tester’s job is to (cid:12)nd pre-release faults,
and customer-reported problems are (cid:12)eld faults that have es-
caped system testing. Any MR in this restricted set whose
Bug-id (cid:12)eld either said bug or was left blank was considered
a fault. If the Bug-id (cid:12)eld said not a bug, the MR was of
If the Bug-id (cid:12)eld’s
course considered not to be a fault.
value was don’t know, we based the decision on a careful
reading of the MR’s detailed description. There were only a
few of these, and all turned out to be faults. All system-test-
found or customer-found MRs that were created before the
bug-identi(cid:12)cation (cid:12)eld was available were considered faults.
As with the earlier studies, the goal of the present study
is to predict the number of faults that executable (cid:12)les of the
system will contain in the next release. Applying the orig-
inal model to a software project requires the existence of a
regular sequence of releases, since the model’s predictions
are based on the fault and change history of (cid:12)les in those
releases. This was appropriate for the inventory and provi-
sioning systems, which used a discrete release development
process, releasing versions at regularly scheduled intervals.
In contrast, the voice response project used a continuous re-
lease process, in which changes are continuously made and
tested, and the changed system is passed on to customers
immediately after it has been tested. Therefore we had to
come up with a way to measure the fault and change his-
tory and to create a de(cid:12)nition of (cid:2)faults for the next release(cid:1)
that would be appropriate to this type of continuous release
schedule. Our solution to the lack of explicit discrete re-
leases is described in the next section.

In light of the voice response system’s di(cid:11)erent develop-
ment process, we were not sure whether the original pre-
diction model, even if it could be applied, would produce
useful results for this system. There are several reasons for
this. In the discrete release model, developers are generally
given a date signi(cid:12)cantly before the scheduled release date
to complete their modi(cid:12)cations and initialization of code,
providing su(cid:14)cient time for system testers to thoroughly
test the code, as well as time for the developers to (cid:12)x any
faults identi(cid:12)ed during testing and then to retest the code.
Also, in the discrete release model, substantial amounts of
new and modi(cid:12)ed code are typically inserted into the system
at the same time. Therefore, many development organiza-

tions maintain large regression test suites, which are rerun
to make sure that all of the changes made to the software
have not caused it to regress, in the sense that things that
worked properly before changes and/or enhancements, no
longer work properly now. Regression testing is typically
done just prior to a new release.

The voice response project also took testing very seriously,
but it was done on a continuous basis as the software was
changed, and hence was more integrated into the develop-
ment process.

4. DEVELOPING SYNTHETIC RELEASES
Our (cid:12)rst attempt at dealing with the lack of regular project
releases was to designate successive three month periods as
synthetic quarterly (cid:2)releases(cid:1). Since these periods are not
really releases, we will refer to them in the sequel as quar-
ters. Under this de(cid:12)nition, (cid:12)les that were initialized any
time during a given three month period would be associated
with that synthetic quarter. However, this would mean that
new (cid:12)les could be in their (cid:12)rst quarter for anywhere be-
tween one day (if they were initialized on the last day of
the quarter) and 92 days (if they were initialized on the (cid:12)rst
day of the quarter). Obviously, a (cid:12)le that was in a quarter
for only a few days would be far less likely to have a failure
detected in that quarter than one which had been in the sys-
tem from the beginning of the quarter, all else being equal.
Further, the second quarter history would be very di(cid:11)erent
for those two (cid:12)les, since one would be nearly new during its
second quarter, while the other would already have under-
gone nearly three full months of testing and usage.

Based on these considerations, we modi(cid:12)ed the synthetic
releases in two ways. First, we reduced the time period
of the synthetic releases from three months to one month.
Monthly releases o(cid:11)er the advantage of allowing us to test
for (cid:12)ner temporal patterns in terms of when faults occur
relative to (cid:12)le creation and either changes or detection of
prior faults.

Second, we modi(cid:12)ed the synthetic releases to avoid initial
release periods that were just a few days long. Because a
(cid:12)le may be added to the system on any day of the calendar
month, the duration of that (cid:12)le in the system for its initial
release period is variable. If a (cid:12)le existed at least one half
of the days in its initial calendar month (e.g., 14 days in a
non-leap year February or 16 days in March), that month is
considered the (cid:12)le’s initial month, and its change and fault
data are included in the data for that month. However, if
a (cid:12)le existed for less than one half of the calendar month,
its data for that month are combined with data for the sub-
sequent month, and this combined entity is treated as the
(cid:12)le’s initial synthetic release period. Thus the change and
fault data for Month 1 (June 2002) is for (cid:12)les that entered
the system on or before June 16. If a (cid:12)le entered on June 17,
it is considered a new (cid:12)le in Month 2 (July 2002), and all of
its changes and faults from June are included in the Month
2 data. Consequently, the initial synthetic release period for
a (cid:12)le can contain anywhere from 14 to 46 days. This period
of time, called the exposure of a new (cid:12)le, is expressed as a
fraction of a month (varying from about 0.5 to 1.5), when it
is used as a variable in the prediction model. In the rest of
the paper, we refer to all the periods of a (cid:12)le’s existence as
months, including the initial variable-length period.

Table 1 provides information on the voice response sys-
tem for each of the 29 calendar months that were tracked.

A total of 1928 (cid:12)les entered the system at some point over
the 29 months, 1926 of which remained through the last
month (one (cid:12)le was dropped after each of months 12 and
20). Over the 29 months studied, a total of 1482 faults were
detected, mostly during system testing. With 329,070 lines
in its latest version, the voice response system is comparable
in size to the systems considered in our earlier studies [18,
19]. The rightmost column of Table 1 shows the monthly
fault density (faults detected per thousand LOC) of the sys-
tem. Although we do not analyze fault density in this paper,
we provide these (cid:12)gures here to permit comparisons with the
fault densities of the earlier systems studied. To maintain
comparability with fault densities reported in [18, 19] and
elsewhere, we adjust fault densities to correspond to rates
per quarter (three months), that is, fault density = faults
per KLOC-quarter. For example, the fault density shown
for Month 2 is

3 (cid:2) 116
59; 371=1000

= 5:86

5. MODELING THE NUMBER OF FAULTS

IN A MONTH

5.1 The Negative Binomial Regression Model
Negative binomial regression is an extension of linear re-
gression designed to handle outcomes that are nonnegative
integers(cid:8)such as the number of faults in a (cid:12)le during a spec-
i(cid:12)ed period [11]. In contrast to Poisson regression, which
assumes that faults occur (cid:2)at random(cid:1) at a rate explained by
a set of predictor variables, the negative binomial model al-
lows for some degree of additional variability in fault counts
that is not explained by any of the available predictor vari-
ables, technically referred to as over dispersion.

Because we want to predict the number of faults discov-
ered for an individual (cid:12)le in a speci(cid:12)c month, the unit of
analysis is the combination of a (cid:12)le and a month. Con-
sequently, a single (cid:12)le may contribute many observations,
corresponding to the number of months it is in the system.
Let observation i refer to a speci(cid:12)c combination of (cid:12)le and
month. We let yi denote the number of faults observed and
xi be the vector of (cid:12)le characteristics for that month. Like
Poisson regression, negative binomial regression models the
logarithm of the expected number of faults as a linear com-
bination of the explanatory variables. That is, the model
assumes that the expected number of faults varies in a mul-
tiplicative way with (cid:12)le characteristics, rather than in an
additive relationship.

The negative binomial regression model speci(cid:12)es that yi
has a Poisson distribution with mean (cid:21)i = (cid:13)ie(cid:12)0xi . A ran-
dom variable (cid:13)i, drawn from a gamma distribution with
mean 1 and unknown variance (cid:27)2 (cid:21) 0, is included to ac-
count for the additional dispersion (variation in the number
of faults) observed in the data. Its variance (cid:27)2, known as
the dispersion parameter, measures the magnitude of the
unexplained concentration of faults.

The regression coe(cid:14)cients (cid:12) and the dispersion parameter
(cid:27)2 are typically estimated by maximum likelihood [11]. All
estimation was performed using Version 9.1 of SAS [22].
Once a model has been estimated, it is simple to compute the
predicted numbers of faults for (cid:12)les in subsequent months,
as long as the same set of explanatory variables is measured
for all (cid:12)les in the new month. These predictions may be

Faults

Fault

Detected Density

Month
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29

Number of
Files
61
325
433
611
758
873
959
1060
1203
1281
1312
1334
1404
1468
1597
1756
1788
1807
1809
1821
1825
1825
1846
1866
1887
1900
1906
1921
1926

Lines Mean Changes
LOC
257.5
182.7
186.2
175.3
173.8
173.2
168.8
164.4
168.5
164.0
165.7
171.8
171.7
171.9
166.2
169.3
168.5
168.5
168.4
168.0
168.0
168.1
169.7
169.6
169.5
169.4
170.3
170.0
170.9

of Code
15,706
59,371
80,619
107,131
131,742
151,232
161,892
174,267
202,700
210,123
217,451
229,152
241,127
252,414
265,380
297,325
301,189
304,435
304,702
306,019
306,688
306,765
313,276
316,422
319,854
321,782
324,533
326,518
329,070

Made
19
257
237
310
398
247
327
302
185
289
159
177
252
174
174
271
160
54
26
36
107
41
129
89
85
96
56
42
71

3
116
29
119
119
100
82
91
32
121
97
89
81
71
40
62
23
9
22
4
52
12
29
32
25
11
2
7
2

0.57
5.86
1.08
3.33
2.71
1.98
1.52
1.57
0.47
1.73
1.34
1.17
1.01
0.84
0.45
0.63
0.23
0.09
0.22
0.04
0.51
0.12
0.28
0.30
0.23
0.10
0.02
0.06
0.02

Table 1: Voice Response System Data By Month

used to prioritize (cid:12)les in terms of their expected number of
faults, which can then be used for purposes of testing.

5.2 Predictor Variables that were Evaluated
Our analysis of fault-proneness relies on a set of simple,
readily-accessible variables that do not depend on the set of
programming languages in use. We utilized a set of dynamic
and some static (cid:12)le characteristics: the age (time in system)
of a (cid:12)le in a given month; the recent history of faults and
changes; programming language; and the current size of a
(cid:12)le, measured simply as lines of code. In addition, we control
for month, or equivalently, for the age of the system.

Graves et al. found that the frequency of faults for a spe-
ci(cid:12)c (cid:12)le decreased with the age of the (cid:12)le [6]. We explore
a variety of relationships with (cid:12)le age. How much more
fault-prone were new (cid:12)les (i.e., (cid:12)les that were introduced to
the system in the current month) compared with those that
also existed in previous releases? After a (cid:12)le is introduced,
does any decrease in faults occur primarily in the (cid:12)rst few
months, or is there a gradual decrease over many following
months?

For existing (cid:12)les, the time at risk for fault discovery is
always one calendar month. However, because (cid:12)les can be
added to the system on any date, the duration of exposure
for new (cid:12)les varies from roughly 0.5 to 1.5 months. Because
the negative binomial model is multiplicative in the predic-
tors, we parameterize exposure for new (cid:12)les as the logarithm

of the fraction of a month at risk (positive if more than a
month, negative if less than a full month). For existing (cid:12)les,
this predictor variable always equals zero.

We found in [18] that existing (cid:12)les in the inventory sys-
tem that were changed in the immediately previous release
(three months earlier) had about three times as many faults
as unchanged (cid:12)les (holding all else equal). In addition to
comparing changed and unchanged (cid:12)les, we explore whether
predictions are improved by counting the number of times
that a (cid:12)le was changed. Because some (cid:12)les were changed a
large number of times in the voice response system (some-
times up to 18 times in a single month), we also assessed the
square root of the number of changes.

In addition, we look at the history of changes to assess
whether changes in the prior month are more important than
changes two or three or up to six months before. We utilize
similar variables to address whether information about a
(cid:12)le’s recent fault history improves predictions alone or in
combination with information about change history.

We include code mass in the model by using the logarithm
of LOC (including comments). If faults occur proportional
to LOC (i.e., fault density is constant with respect to LOC),
then the coe(cid:14)cient for log(LOC) should be approximately
1.0.

As in [18] and [19], we restrict the fault analysis and pre-
dictions to (cid:12)les that are normally written and maintained
by programmers (i.e., source (cid:12)les), and that are executable.

Files are selected on the basis of their extension. Thus,
for example, .java, .xml, and .jsp (cid:12)les are included in the
study, while .doc (cid:12)les are not included since they are not
executable, and .jpg and .wav (cid:12)les are not included because
they are not produced by a programmer. In [19], we found
that fault-proneness di(cid:11)ered signi(cid:12)cantly among program-
ming languages (e.g., java, sql, etc.) even after controlling
for lines of code and other (cid:12)le characteristics. We test for
similar di(cid:11)erences using dummy variables for the most com-
mon languages in the voice response system. Because there
were 34 distinct languages in use by the last months under
study, we group less common languages based on average
code mass.

As Table 1 illustrates, fault density falls dramatically,
though unevenly, from the early months to the later months
of the system. While much of that decline may be explained
by other variables such as the age and recent change history
of individual (cid:12)les, there may still be systematic di(cid:11)erences
associated with di(cid:11)erent months. To control for any such
di(cid:11)erences, we include dummy variables for each month in
our primary models.

6. VARIABLE SELECTION FOR THE FAULT

MODELS

We now describe the procedures for selecting the models
whose predictions are evaluated in the next section. Se-
lection of predictor variables involves both decisions about
what factors to include in the negative binomial regression
model and, for some factors, the appropriate time frame(s)
and transformations.

Variable selection seldom produces an obvious best set of
predictors. Rather than use rote optimization criteria, we
used our judgment and past experience to guide the process.
However, we did take two steps to ensure that assessment of
the model predictions (in Section 7) is based on data that
were independent of those data used for variable selection.
First, we (cid:12)xed the speci(cid:12)cation of the predictor variables
based on analysis through Month 9 only. Consequently, re-
sults from Months 10 onward could not a(cid:11)ect the set of
predictors used for those months.

Second, we based variable selection on a training sample
consisting of a random sample of 1327 (cid:12)les (each (cid:12)le was
independently selected at random with probability 2/3), re-
sulting in 4342 observations (combinations of (cid:12)le and month).
This allowed predictions to be assessed on a validation sam-
ple of 601 (cid:12)les that were not used at all for either the variable
selection or for coe(cid:14)cient estimation. If over (cid:12)tting to the
training sample led to including too many variables in the
model, that should show up in poor predictive performance
for the validation sample.

6.1 Results for Months 1 to 9

Table 2 presents results for the main model (referred to
as the Basic Model) that came out of the variable selection
process with data for Months 1 to 9. The sign of an esti-
mated coe(cid:14)cient indicates the direction of the relationship
between the value of the corresponding predictor variable
and the expected number of faults, holding all other pre-
dictor variables (cid:12)xed. For continuous predictors, the co-
e(cid:14)cient estimate indicates the change in the logarithm of
the expected number of faults associated with a unit change
in the value of the predictor variable. This can be trans-

lated into a multiplicative change in the expected number
of faults by exponentiating the coe(cid:14)cient. For example, a
unit increase in the square root of changes in the past month
is associated with a multiplicative increase of e0:654 = 1.92
times more faults. For categorical predictor variables, each
coe(cid:14)cient estimates the relative di(cid:11)erence in the logarithm
of the expected number of faults for the corresponding cate-
gory versus a reference category where all dummy variables
are zero.

One of the most important single predictors was the log-
arithm of lines of code. For the inventory system, the es-
timated coe(cid:14)cient of 1.05 did not di(cid:11)er signi(cid:12)cantly from
1.00, so that the model for that system was consistent with
the number of faults being proportional to LOC (holding
all else equal) [19]. For the provisioning system, the esti-
mated coe(cid:14)cient was signi(cid:12)cantly lower than 1.00, at 0.73
[19]. However, for the voice response system the estimated
coe(cid:14)cient for log(KLOC) was much lower yet, about 0.5.
This result implies that the expected number of faults only
grows something like the square root of LOC (LOC raised
to the 0.5 power), or equivalently, that fault density declines
proportional to the square root of LOC (LOC raised to the
(1-0.5)th power).

Exploratory analysis indicated that fault-proneness de-
creases with the age of a (cid:12)le, even after controlling for other
(cid:12)le characteristics such as recent changes and faults. How-
ever, this relationship was not explained fully by a single
linear term for a (cid:12)le’s age (in months). For the inventory
system, adding a dummy variable for new (cid:12)les improved the
(cid:12)t, by allowing for a relatively larger decline in faults from
the (cid:12)rst to the second release that a (cid:12)le existed [19]. For
the voice response system, there appeared to be nonlinear-
ity beyond the (cid:12)rst month. However, because (cid:12)le age is
highly correlated with month number during the (cid:12)rst nine
months of the system, those data did not allow us to reli-
ably determine the pattern of this relationship. To allow
for additional nonlinearity over the (cid:12)rst few months after
(cid:12)le initialization, we decided to include dummy variables
for a (cid:12)le being in its second, third, fourth, or (cid:12)fth months in
the Basic Model, in addition to a dummy variable for new
(cid:12)les.

Data on prior changes includes counts for all previous
months that a (cid:12)le existed in the system (for any month
before a (cid:12)le entered the system, a value of 0 is imputed).
Because counts of changes for a given month are very skewed
(mostly zero or one, but a few large values), we considered
three alternative forms:
the raw count, the square root,
and a binary indicator of whether there were any changes.
Based on improvements in the log likelihood of the model,
the square root transformation consistently (cid:12)t better than
In addition, we investi-
either of the other two versions.
gated whether only changes from the prior month mattered
or whether a longer history improved predictions. This ex-
ploration suggested that the most recent month was the best
predictor, but that incorporating changes in prior months
also improved the log likelihood of the (cid:12)t, with no clear
pattern distinguishing among those prior months. Conse-
quently, we selected two measures of prior changes:
the
square root of the number of changes in the most recent
month and the square root of the total number of changes
in the (cid:12)ve months prior to the most recent month.

The positive estimated coe(cid:14)cients for these two measures
of prior changes each indicate a positive relationship between

Std.
Coef. Error

.060
.367

.495
.620

.104
.096

.654
.467

.280
1.555
1.290
1.018
.740
.532

-.518
-.641
-1.399
-2.112
-1.144
-1.457

Predictor
LOC and Exposure
log(KLOC)
log(exposure)
Age of File
Age (in months)
New
Second Month
Third Month
Fourth Month
Fifth Month
Square Root of Prior Changes
Past Month
Months 2-6
Program Type
Very Small
Small
js
sh
Month Number
-1.074
1
1.484
2
.497
3
1.677
4
1.381
5
1.147
6
1.199
7
8
1.180
Dispersion Parameter
(cid:27)2
2.278

1.075
.346
.371
.306
.309
.309
.299
.294

1.208
.567
-1
1.093

.301
.195
(cid:9)
.234

.356

95 Percent

t Conf. Interval

8.30
1.69

(.378, .612)
(-.099, 1.340)

-1.85
-.41
-1.08
-2.07
-1.54
-2.74

6.26
4.86

4.02
2.91
(cid:9)
4.67

-1.00
4.29
1.34
5.49
4.47
3.71
4.02
4.02

(-1.066, .030)
(-3.688, 2.406)
(-3.928, 1.131)
(-4.108, -.116)
(-2.595, .307)
(-2.500, -.414)

(.449, .858)
(.278, .655)

(.618, 1.797)
(.186, .949)
(cid:9)
(.634, 1.552)

(-3.180, 1.033)
(.806, 2.161)
(-.229, 1.224)
(1.078, 2.276)
(.775, 1.986)
(.541, 1.753)
(.614, 1.784)
(.605, 1.756)

6.40

(1.580, 2.976)

Table 2: Negative Binomial Regression Results for the Basic Model, Months 1 to 9

the number of changes (on the square root scale) and the ex-
pected number of faults, while all other variables are (cid:12)xed.
Furthermore, the sizes of the estimated coe(cid:14)cients provide
us estimates of the magnitude of those relationships. For ex-
ample, the estimated coe(cid:14)cient for the most recent month
implies that even a single change during that month is as-
sociated with a 92% increase (e0:654 = 1.92) in the expected
number of faults relative to a similar (cid:12)le with no changes in
the most recent month.

We conducted a similar analysis to investigate the predic-
tive power of the recent history of faults. For the inventory
system, we had found that including the square root of the
number of faults during the prior release signi(cid:12)cantly im-
proved predictions [19]. Although fault history was useful
in the absence of change history for the voice response sys-
tem, it was much less useful than the change history.
In
addition, once change history was included in the model, no
combination of fault history variables proved statistically
signi(cid:12)cant. Consequently, we do not include any measures
of prior faults in the model.

Simple comparison of fault densities for the various pro-
gramming languages in the system highlights large di(cid:11)er-
ences among languages. Unfortunately, of 34 languages rep-
resented in the system, only a handful occurred often enough
to support accurate (cid:12)tting of language-speci(cid:12)c dummy vari-
ables. Of those, javascript (js) and shell (sh) were clearly
signi(cid:12)cant. For javascript, there were no faults in 203 (cid:12)le-

month combinations and more than 43 KLOC during Months
1 to 9, resulting in a maximum likelihood estimate of -1 for
the js coe(cid:14)cient.
In contrast, the coe(cid:14)cient for sh indi-
cates approximately three times as many faults (e1:093 =
2.98), holding all else equal, for sh (cid:12)les relative to other pro-
gram types (excluding js and languages whose average (cid:12)le
size typically contained very few lines of code). very small
groupings).

Exploratory analysis also showed that fault densities by
programming language tended to be inversely related to the
average length of (cid:12)les of that type. Consequently, we tested
dummy variables for groups of languages where the average
(cid:12)le size was very small (less than 40 LOC) or small (40 to 65
LOC). In comparison with larger (cid:12)le types (excluding js and
sh), each group had signi(cid:12)cantly more faults than otherwise
predicted (after controlling for log(KLOC)).

Finally, the Basic Model includes a series of dummy vari-
ables contrasting fault rates in Months 1 to 8 against Month
9. Positive coe(cid:14)cients for Months 2 through 8 re(cid:13)ect the
relatively higher fault densities in those months compared
with that for Month 9 (see Table 1). The only negative
coe(cid:14)cient occurred for Month 1, which had only a slightly
higher fault density than Month 9, despite consisting of all
new (cid:12)les.

The (cid:12)nal row of Table 2 shows the estimated dispersion
parameter for the model. Coincidentally, this estimate is
almost identical to the estimated value of 2.27 reported for

the (cid:2)Full Model(cid:1) for the (cid:12)rst 12 releases of the inventory
system [17].

In addition to the variables identi(cid:12)ed above as statis-
tically signi(cid:12)cant, our model development work identi(cid:12)ed
some other program type dummy variables and interactions
that seemed to improve the (cid:12)t for Months 1 to 9. How-
ever, because these terms were generally estimated from
small slices of the data, we did not feel con(cid:12)dent enough
to include them in the Basic Model. The (cid:12)rst set includes
dummy variables for six additional program types: conf,
html, java, jsp, xml, and xsl. Each program type had mod-
erately large and positive estimated coe(cid:14)cients, although
standard errors were also large. The second set included
interactions of log(KLOC) with dummy variables for conf,
html, sh, xml, and the grouping of very small program
types.
In essence, these interactions allow the coe(cid:14)cient
for log(KLOC) to vary by program type. For each of the
(cid:12)ve types listed above (including very small), the end result
was an estimated log(KLOC) coe(cid:14)cient near zero. In other
words, the model detected little or no relationship between
LOC and faults for those program types. Once those inter-
actions were included, the estimated coe(cid:14)cient for all other
program types rose to 0.71.

6.2 Models to be Evaluated

We now de(cid:12)ne the models whose predictions we will assess

in the next section.

6.2.1 The Basic Model
Based on the analyses just described for Months 1 to 9, we
speci(cid:12)ed the variables for inclusion in our primary prediction
model, which we call the Basic Model. The Basic Model
uses the following predictor variables: the size of the (cid:12)le in
terms of the log of KLOC; the logarithm of exposure (the
proportion of the month the (cid:12)le was in the system); the age
of the (cid:12)le (the number of full months the (cid:12)le was in the
system); indicators for whether the (cid:12)le was new, one, two,
three, or four months old; the square root of the number of
changes made during the prior month; the square root of
the total number of changes made during the (cid:12)ve months
indicators for (cid:12)les written in
preceding the prior month;
javascript (js) or shell (sh); and indicators of whether the
(cid:12)le was written in a language for which the average (cid:12)le size
was either very small or small, as de(cid:12)ned above. The very
small category includes (cid:12)le types cls, pl, and cron, among
others, while the small category includes sql, xml, and vxml.
In addition, the model included a series of dummy variables
for all but one month represented in the analysis.

Although similar to the model we used in [19], the set of
predictor variables used in this model di(cid:11)ers in important
ways. Because [19] analyzed quarterly releases for the in-
ventory system, it did not use as detailed a pro(cid:12)le of (cid:12)le
age or as detailed a history of changes (only whether there
were any changes during the prior release). On the other
hand, [19] incorporated faults in the prior release, which we
found unnecessary for the voice response system. Also, be-
cause the mixture of programming languages used in the
voice response system was much more extensive than that
for the inventory system, we needed to aggregate many of
the less common languages (we chose to group by average
LOC) in order to avoid over(cid:12)tting.

To investigate how well this model, which is based on
only the (cid:12)rst nine months of data, applies to later months,

we re(cid:12)t it on data from Months 10 to 28. We excluded
Month 29 from the analysis because it only contained a sin-
gle fault in the training data. Results might change either
if the variable selection involved over (cid:12)tting (incorrect in-
clusion of variables based on spurious association) or if the
(cid:2)correct(cid:1) model systematically changed as the system aged.
Table 3 presents regression results for the Basic Model (cid:12)t
to data from the training sample (cid:12)les for Months 10 to 28
(22,298 observations). Although this model included a series
of 18 dummy variables for Months 10 to 27 (with Month 28
serving as the reference category), we omit those coe(cid:14)cients
from the table because they have little intrinsic interest.

The results for Months 10 to 28 are generally similar to
those observed for Months 1 to 9, except for the Age-of-File
variables, which could not be stably estimated from nine
months of data. The coe(cid:14)cient for log(KLOC) fell even
further, to 0.38, but the values for the two periods did not
di(cid:11)er signi(cid:12)cantly. Although the estimated coe(cid:14)cient for
log(exposure) tripled in the later period, both estimates were
consistent with a value of 1.00 (recall that this explanatory
variable varies only for new (cid:12)les, so there was little informa-
tion for estimating this coe(cid:14)cient precisely in Months 10 to
28).

The coe(cid:14)cients associated with the ages of (cid:12)les are all es-
timated much more precisely (i.e., with much smaller stan-
dard errors) for the later set of months and consequently
are easier to interpret. The linear coe(cid:14)cient for (cid:12)le age is
-0.094, indicating that the expected number of faults de-
cays by an estimated 9% each month (after the (cid:12)le’s (cid:12)fth
month). The coe(cid:14)cient for new (cid:12)les indicates a very strong
increase in faults associated with (cid:12)les in their (cid:12)rst month
(analogous to a similar (cid:12)nding in [19] for (cid:12)les in their (cid:12)rst
three-month release).
In addition, the coe(cid:14)cient for the
second month suggests that this phenomenon lasts, in part,
for two months. On the other hand, the coe(cid:14)cient for the
third month shows a surprising association in the opposite
direction.

Coe(cid:14)cients for the prior change variables look very similar
in both periods. The same is generally true for the program
type coe(cid:14)cients, although faults did occur for javascript (cid:12)les
during the later periods, so that the js coe(cid:14)cient is only
moderately negative and not statistically signi(cid:12)cant.

6.2.2 The Enhanced Model
Exploratory analysis of the data for Months 1 to 9 sug-
gested some other relationships that we were not as con-
(cid:12)dent of: dummy variables for a few other common pro-
gramming languages and interactions between log(KLOC)
and selected dummy variables for programming language.
Rather than include those predictors in the Basic Model,
we designated a so-called Enhanced Model to include in the
model assessments in Section 7. The Enhanced Model in-
cludes all of the factors in the Basic Model plus: dummy
variables for conf, html, java, jsp, xml, and xsl; and interac-
tions of log(KLOC) with each of conf, html, sh, xsl, and the
very small grouping.

When corresponding models were (cid:12)t to the data from
Months 10 to 28, coe(cid:14)cients for all six new dummy vari-
ables fell in size, suggesting that inclusions of those factors
may involve over (cid:12)tting. In contrast, results for four of the
(cid:12)ve interactions with LOC (all but for xsl) followed the same
pattern as in Months 1 to 9.

Std.
Coef. Error

.055
.610

.378
1.936

.020
.302
.303
.374
.281
.278

-.094
1.965
.725
-.902
.121
-.190

Predictor
LOC and Exposure
log(KLOC)
log(exposure)
Age of File
Age (in months)
New
Second Month
Third Month
Fourth Month
Fifth Month
Square Root of Prior Changes
Past Month
Months 2-6
Program Type
.474
Very Small
.442
Small
-.907
js
sh
.834
Dispersion Parameter
(cid:27)2
6.454

.313
.184
.753
.204

1.037
.642

.123
.073

.836

95 Percent

t Conf. Interval

6.86
3.18

(.270, .486)
(.741, 3.131)

-4.82
6.51
2.39
-2.41
.43
-.68

(-.133, -.056)
(1.374, 2.557)
(.132, 1.318)
(-1.634, -.169)
(-.430, .672)
(-.736, .355)

8.46
8.80

(.797, 1.277)
(.499, .785)

1.52
2.41
-1.20
4.09

(-.139, 1.087)
(.082, .802)
(-2.383, .569)
(.435, 1.233)

7.72

(4.816, 8.092)

Table 3: Negative Binomial Regression Results for the Basic Model, Months 10 to 28

6.2.3 The Simpliﬁed Model
We also designate a Simpli(cid:12)ed Model, which uses the
following information: the size of the (cid:12)le in terms of the
log of KLOC; whether the (cid:12)le was initialized during this
if not, whether it was changed during the prior
month;
month; the month as a re(cid:13)ection of the system’s age; and
the log(exposure) variable. Assessment of predictions from
the Simpli(cid:12)ed Model helps to tell us whether the additional
complexities of the Basic and Enhanced Models improve pre-
dictions su(cid:14)ciently to be worthwhile.

6.2.4 The LOC Model
The LOC Model uses only a count of the number of lines
of code in a (cid:12)le. The (cid:12)les are simply ordered by decreasing
number of lines, and the appropriate number of (cid:12)les are
selected to reach a pre-determined percentage of the (cid:12)les.
As with our earlier studies, we consider the number of faults
contained in 20% of the (cid:12)les predicted to contain the largest
numbers of faults. This simple model proved surprisingly
e(cid:11)ective in predicting fault-prone (cid:12)les for the inventory and
provisioning systems in our earlier studies. We therefore
include this model in our current study for completeness.

7. PREDICTION RESULTS

In this section, we assess the prediction models described
above, by quantifying how well each model is able to predict
prospectively where faults will occur. Speci(cid:12)cally, for Month
N, we use training data from Months 1 to (N-1) to estimate
coe(cid:14)cients for predictive models, which are used to predict
the number of faults for every (cid:12)le in the system at Month N.
After sorting the (cid:12)les from most to least fault-prone, based
on each model’s predictions, we compute the percentage of
faults contained in the top 20% of (cid:12)les for each ordering.

Although estimated coe(cid:14)cients for each model are based
on data that do not overlap the assessment data, there may
be concern about bias in the assessment results if the assess-

ment uses the same (cid:12)les that variable selection was based on.
Assessment of predictions for the validation sample avoids
that bias because none of those (cid:12)les overlap the training
sample (cid:12)les used for variable selection.
It turns out that
prediction results for the validation sample were generally
better than those for the training sample, so that bias does
not seem to have been a problem. Consequently, we report
results in this section for the complete set of (cid:12)les.

Table 4 shows prediction results for the four models. To
reduce variability resulting from small numbers of faults in
some months, especially later months, we aggregate the (cid:12)nd-
ings for sets of three consecutive months. The (cid:12)rst row of
the table shows results from Months 7, 8, and 9. We con-
tinue aggregating every 3 successive months through Months
25 to 27. Months 28 and 29 are not included in the analysis
because there are insu(cid:14)cient faults for meaningful analysis.
Values were computed by adding together the total num-
ber of faults included in the 20% of the (cid:12)les identi(cid:12)ed by
a given model during the three month period and dividing
the results by the total number of faults actually detected
by testers and users during the three months. So, for exam-
ple, during Months 7, 8 and, 9, there were 82, 91, and 32
faults, respectively for a total of 205 faults during the three
months (see Table 1). Of those faults, the Basic Model iden-
ti(cid:12)ed (cid:12)les containing 55 in Month 7, 47 in Month 8, and 23
in Month 9, for a total of 125. Therefore, for Months 7 to
9, Table 4 shows that the 20% of the (cid:12)les identi(cid:12)ed by the
Basic Model contained 125/205 = 61.0% of the faults.

The (cid:12)rst row of Table 4 shows that for Months 7 to 9, all
four models included about the same percentage of faults
(60% to 63%) in the top 20% of (cid:12)les. However, results di-
verged in later months. For the LOC Model, performance
trended fairly (cid:13)at, but with a dip in Months 13 to 18. In
contrast, performance for the Basic and Enhanced Models
generally improved with time, with each model surpassing
70% for most three-month aggregates. The Simpli(cid:12)ed Model
almost always fell somewhere between these two extremes.

Months
7-9
10-12
13-15
16-18
19-21
22-24
25-27
Weighted
Unweighted

Faults

LOC
Detected Model
60.0
58.6
46.9
37.2
62.8
67.1
57.9
55.5
55.8

205
307
192
94
78
73
38
987
(cid:9)

Simpli(cid:12)ed Basic
Model
61.0
65.5
79.2
70.2
73.1
97.3
78.9
71.1
75.0

Model
61.5
63.2
68.2
53.2
69.2
87.7
68.4
65.3
67.3

Enhanced
Model
62.9
69.1
75.0
67.0
70.5
95.9
84.2
71.4
74.9

Table 4: Percentage of Faults Detected in Top 20 Percent of Files, By Three Month Periods

% of Faults
Targeted
60
70
80
90
95

Inventory System Provisioning System Voice System (Basic Model)
% of Faults
Included
62
71
81
89
95

% of Faults
Included
54
65
76
89
94

% of Faults
Included
60
70
81
89
92

% Files
12
20
32
52
67

% Files
8
12
18
31
43

% Files
6
9
15
27
41

Table 5: Targeted and Included Faults for Inventory, Provisioning, and Voice Response Systems

The (cid:12)nal two rows of Table 4 show averages over time,
computed in two distinct ways. The row labeled (cid:2)Weighted(cid:1)
shows averages for each model weighted by the numbers of
faults in each three-month period. Overall, the Basic and
Enhanced Models performed best and were nearly indistin-
guishable. The Simpli(cid:12)ed Model performed less well, but
was in the ballpark, while the LOC model lagged badly.
The (cid:2)Unweighted(cid:1) row shows simple averages of the per-
centages in each column to allow equitable comparison with
numbers reported in [19] for the inventory system. This
row shows that both the Basic and Enhanced Models iden-
ti(cid:12)ed 75% of faults on average. By comparison, the most
comparable model for the inventory system had averaged
83%. A much larger gap of 18 percentage points occurred
for the LOC Model, which averaged only 55% for the voice
response system versus 73% for the inventory system [19].
In [20], we also viewed the data from a di(cid:11)erent perspec-
tive: comparing the percentage of faults actually detected in
a set of (cid:12)les versus the percentage of faults predicted to be in
those (cid:12)les. Table 5 summarizes prior results of this sort and
new (cid:12)ndings for the voice response system, averaged across
all releases or periods. For the inventory system, for exam-
ple, when (cid:12)les were selected with the goal of containing 70%
of all faults, it required 9% of (cid:12)les, which actually contained
65% of faults. Using the same target for the provisioning
system, the model required 12% of the (cid:12)les, which included
71% of the faults. When viewed this way, predictions for the
voice response system are again very accurate, although a
substantially larger fraction of the (cid:12)les is needed to reach a
given percentage of faults than for the earlier systems. For
instance, if the goal is to capture at least 80% of the faults in
the selected (cid:12)les, fewer than 20% of the (cid:12)les are needed, on
average, for both the inventory system and the provisioning
system. For the voice response system, roughly a third of
the (cid:12)les are needed, making the prediction somewhat less
valuable for practitioners.

8. CONCLUSIONS

We have performed an empirical study of software fault
prediction for an industrial voice response system, follow-
ing the system over a two and one half year period. Be-
cause this project did not release new versions of the system
at regularly scheduled intervals, it was unclear whether our
previously developed prediction models would be applicable.
To overcome the project’s lack of regularly scheduled soft-
ware releases, we aggregated detected faults into time peri-
ods de(cid:12)ned by calendar months and constructed a model
that makes predictions of faults for a future time period
based on information from the previous periods. These pe-
riods, called synthetic releases, are variable-length months
ranging from 14 to 46 days for the (cid:12)rst time period that
a (cid:12)le enters the system, and calendar months thereafter.
With this scheme, we were able to make fault predictions
that should be of value to practitioners.

We looked at several di(cid:11)erent models and found that the
two we called the Basic and Enhanced Models performed
best. When the months were aggregated into quarters, cov-
ering the period from June 2002 through August 2004, the
20% of the (cid:12)les selected by the Basic Model as being most
likely to contain the largest numbers of faults contained, on
average, 75% of the actual faults.

At a high level, the (cid:12)ndings of this study con(cid:12)rm those
from our two previous software systems. Similar sets of vari-
able are statistically signi(cid:12)cant in negative binomial regres-
sion models to predict software faults. Although the aver-
ages for both of the systems investigated during earlier stud-
ies averaged 83% for the top 20% of the (cid:12)les, we consider
the current results very encouraging. In particular, they im-
ply that we are able to help developers and testers identify
(cid:12)les that need to be targeted and to help prioritize testing
activities even if their project does not follow a process that
uses regularly scheduled releases.

As expected, notable distinctions arise between systems.
The most e(cid:11)ective way to account for prior changes di(cid:11)ered
for the new system. In contrast with the inventory system,
information on prior faults was no longer signi(cid:12)cant once the
model had accounted for counts of changes in prior months.
Another new (cid:12)nding for this system was that fault densities
were much higher for program types that are generally small
(less than 65 LOC) than for types that generally exceed that
size.

Perhaps the most substantial di(cid:11)erence was that lines of
code was a much poorer predictor of faults than it had been
for the other systems. Rather than faults occurring roughly
proportional to size (measured as LOC) as they did for the
inventory system, the expected number of faults grew more
like the square root of LOC. This di(cid:11)erence appears to be
behind the poorer predictive performance of the best mod-
els for the voice response system, compared with that for
the inventory and provisioning systems. Given the various
di(cid:11)erences among systems, it is clear that one should not
blindly apply the (cid:2)best(cid:1) model from one system to a new
system without adequate validation.

For the inventory system, we concluded [19] that a LOC-
only model might be a viable alternative to more complex
models because it sacri(cid:12)ced only 10 percentage points in
terms of faults included in the top 20% of (cid:12)les. However,
the gap for the voice response system is about twice that
large on average: 55% versus 75%. Consequently, ignoring
other factors would not be a viable strategy for the voice re-
sponse system. Among the more complete models, the per-
formances for the Basic and Enhanced Models were nearly
indistinguishable, so that there seems little reason to use
the more complex Enhanced Model. Although the Sim-
pli(cid:12)ed Model does sacri(cid:12)ce some degree of accuracy when
compared with the Basic and Enhanced Models, it may be
viewed as a viable alternative given its relative simplicity.

It is, of course, risky to attribute di(cid:11)erent (cid:12)ndings from
earlier studies to particular characteristics of the voice re-
sponse system. However, the (cid:12)ndings of this study are valu-
able because they add to the fairly short list of similar stud-
ies for large industrial software systems. In addition, it does
show that this sort of prediction modeling can be useful for
systems that do not use regularly scheduled releases.

Acknowledgments
We are very grateful to all our colleagues at AT&T without
whose cooperation and assistance this research could not
have been completed. Teresa Irizarry and Dave Morehead
allowed us access to developers, testers, and system adminis-
trators, who in turn helped us understand the system struc-
ture and software development process for the voice response
system. Marcy Braunstein and Jim Wilson were especially
helpful in providing details of the system’s testing and de-
velopment processes. Mike Carr provided crucial assistance
in understanding the version management system and fault
repository and also helped the authors construct scripts to
extract system data. Linda Halperin and Dan Doheny were
also very helpful with these systems and always very gener-
ous with their time. When the version management system
was itself replaced with a new version, Andy Nocera helped
guide us through the transition. Filip Vokolos of Drexel Uni-
versity was instrumental in creating an accurate inventory
of the system’s (cid:12)les and faults.

9. REFERENCES
[1] E.N. Adams. Optimizing Preventive Service of Software

Products. IBM J. Res. Develop., Vol 28, No 1, Jan 1984, pp.
2-14.

[2] V.R. Basili and B.T. Perricone. Software Errors and

Complexity: An Empirical Investigation. Communications of
the ACM, Vol 27, No 1, Jan 1984, pp. 42-52.

[3] G. Denaro and M. Pezze. An Empirical Evaluation of
Fault-Proneness Models. Proc. International Conf on
Software Engineering (ICSE2002), Miami, USA, May 2002.
[4] S.G. Eick, T.L. Graves, A.F. Karr, J.S. Marron, A. Mockus.
Does Code Decay? Assessing the Evidence from Change
Management Data. IEEE Trans. on Software Engineering,
Vol 27, No. 1, Jan 2001, pp. 1-12.

[5] N.E. Fenton and N. Ohlsson. Quantitative Analysis of Faults
and Failures in a Complex Software System. IEEE Trans. on
Software Engineering, Vol 26, No 8, Aug 2000, pp. 797-814.
[6] T.L. Graves, A.F. Karr, J.S. Marron, and H. Siy. Predicting

Fault Incidence Using Software Change History. IEEE Trans.
on Software Engineering, Vol 26, No. 7, July 2000, pp.
653-661.

[7] L. Guo, Y. Ma, B. Cukic, H. Singh. Robust Prediction of
Fault-Proneness by Random Forests. Proc. ISSRE 2004,
Saint-Malo, France, Nov. 2004.

[8] L. Hatton. Reexamining the Fault Density - Component Size
Connection. IEEE Software, March/April 1997, pp. 89-97.

[9] T.M. Khoshgoftaar, E.B. Allen, K.S. Kalaichelvan, N. Goel.

Early Quality Prediction: A Case Study in
Telecommunications. IEEE Software, Jan 1996, pp. 65-71.
[10] T.M. Khoshgoftaar, E.B. Allen, J. Deng. Using Regression

Trees to Classify Fault-Prone Software Modules. IEEE Trans.
on Reliability, Vol 51, No. 4, Dec 2002, pp. 455-462.

[11] P. McCullagh and J.A. Nelder. Generalized Linear Models,

Second Edition, Chapman and Hall, London, 1989.
[12] A. Mockus and D.M. Weiss. Predicting Risk of Software

Changes. Bell Labs Technical Journal, April-June 2000, pp.
169-180.

[13] K-H. Moller and D.J. Paulish. An Empirical Investigation of
Software Fault Distribution. Proc. IEEE First International
Software Metrics Symposium, Baltimore, Md., May 21-22,
1993, pp. 82-90.

[14] J.C. Munson and T.M. Khoshgoftaar. The Detection of
Fault-Prone Programs. IEEE Trans. on Software
Engineering, Vol 18, No 5, May 1992, pp. 423-433.

[15] N. Ohlsson and H. Alberg. Predicting Fault-Prone Software
Modules in Telephone Switches. IEEE Trans. on Software
Engineering, Vol 22, No 12, December 1996, pp. 886-894.
[16] T. Ostrand and E.J. Weyuker. The Distribution of Faults in a
Large Industrial Software System. Proc. ACM/International
Symposium on Software Testing and Analysis (ISSTA2002),
Rome, Italy, July 2002, pp. 55-64.

[17] T. Ostrand, E.J. Weyuker, and R.M. Bell. Using Static

Analysis to Determine Where to Focus Dynamic Testing
E(cid:11)ort. Proc. IEE/Workshop Dynamic Analysis (WODA 04),
May 2004.

[18] T.J. Ostrand, E.J. Weyuker, and R.M. Bell. Where the Bugs
Are. Proc. ACM/International Symposium on Software
Testing and Analysis (ISSTA2004), Boston, MA, July 2004.

[19] T.J. Ostrand, E.J. Weyuker, and R.M. Bell. Predicting the
Location and Number of Faults in Large Software Systems.
IEEE Trans. on Software Engineering, Vol 31, No 4, April
2005.

[20] T.J. Ostrand, E.J. Weyuker, and R.M. Bell. A Di(cid:11)erent View

of Fault Prediction. Proc. 29th IEEE Annual International
Computer Software and Applications Conference
(COMPSAC 2005), Edinburgh, Scotland, July, 2005.
[21] M. Pighin and A. Marzona. An Empirical Analysis of Fault
Persistence Through Software Releases. Proc. IEEE/ACM
ISESE 2003, pp. 206-212.

[22] SAS Institute Inc. SAS/STAT 9.1 User’s Guide, SAS

Institute, Cary, NC, 2004.

